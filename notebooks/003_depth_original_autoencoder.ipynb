{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T07:18:34.383481Z",
     "iopub.status.busy": "2024-12-06T07:18:34.383127Z",
     "iopub.status.idle": "2024-12-06T07:18:34.388452Z",
     "shell.execute_reply": "2024-12-06T07:18:34.387556Z",
     "shell.execute_reply.started": "2024-12-06T07:18:34.383450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    project_name = \"clarion-ai-003\"\n",
    "\n",
    "    compression_ratio = 0.9\n",
    "    channels = 32\n",
    "\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 500\n",
    "    wandb_api_key = os.environ.get(\"WANDB_API_KEY\", \"\")\n",
    "    dataset_path = \"/input/speechocean762/train/*.wav\"\n",
    "    pretrained_model_path = \"/input/clarion-ai-002-ds/speech_autoencoder.pth\"\n",
    "    model_save_path = \"speech_autoencoder_en_score.pth\"\n",
    "    whisper_path = \"openai/whisper-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T07:18:34.771544Z",
     "iopub.status.busy": "2024-12-06T07:18:34.771207Z",
     "iopub.status.idle": "2024-12-06T07:19:10.306911Z",
     "shell.execute_reply": "2024-12-06T07:19:10.306086Z",
     "shell.execute_reply.started": "2024-12-06T07:18:34.771514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import inspect\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import wandb\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, WhisperTokenizer\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    lengths = [item.size(1) for item in batch if item.numel() > 0]  # Skip empty tensors\n",
    "    max_length = max(lengths)\n",
    "    padded_batch = torch.zeros(len(batch), 1, max_length, dtype=batch[0].dtype)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        if item.numel() > 0:\n",
    "            padded_batch[i, :, : item.size(1)] = item\n",
    "\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sample_rate = torchaudio.load(self.file_paths[idx])\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return audio\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, compression_ratio=0.7, channels=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.compression_ratio = compression_ratio\n",
    "\n",
    "        # Three convolutional layers with group normalization and ReLU activation\n",
    "        # Increase the number of feature maps to improve representational capacity\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(channels, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.encoder_layers(x)\n",
    "        # Compress the time dimension\n",
    "        compressed_length = int(x.size(2) * self.compression_ratio)\n",
    "        x = nn.functional.interpolate(x, size=compressed_length, mode=\"linear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, expansion_ratio=1 / 0.7, channels=16):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.expansion_ratio = expansion_ratio\n",
    "\n",
    "        # Three transposed convolutional layers with group normalization and ReLU activation\n",
    "        # Mirror the encoder structure\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose1d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose1d(channels, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            # Note: Final activation can be omitted or chosen based on the output signal's nature\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the time dimension\n",
    "        original_length = int(x.size(2) * self.expansion_ratio)\n",
    "        x = nn.functional.interpolate(x, size=original_length, mode=\"linear\", align_corners=False)\n",
    "        # Pass through the transposed convolutional layers\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, compression_ratio=0.7, channels=16):\n",
    "        super(SpeechAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(compression_ratio=compression_ratio, channels=channels)\n",
    "        self.decoder = Decoder(expansion_ratio=1 / compression_ratio, channels=channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "\n",
    "        # print(x.shape, x.numel())\n",
    "        # print(latent.shape, latent.numel())\n",
    "        # print(reconstructed.shape, reconstructed.numel())\n",
    "        # print(\"-\"*30)\n",
    "\n",
    "        # if reconstructed.size(2) > x.size(2):\n",
    "        #     reconstructed = reconstructed[:, :, :x.size(2)]\n",
    "        # elif reconstructed.size(2) < x.size(2):\n",
    "        #     pad_length = x.size(2) - reconstructed.size(2)\n",
    "        #     reconstructed = nn.functional.pad(reconstructed, (0, pad_length))\n",
    "        return reconstructed\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path, device=\"cpu\", **kwargs):\n",
    "        init_params = inspect.signature(cls).parameters\n",
    "        init_kwargs = {\n",
    "            key: param.default\n",
    "            for key, param in init_params.items()\n",
    "            if param.default is not inspect.Parameter.empty and key != \"self\"\n",
    "        }\n",
    "        init_kwargs.update(kwargs)\n",
    "        model = cls(**init_kwargs).to(device)\n",
    "        state_dict = torch.load(model_path, map_location=device, weights_only=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def _load_state_dict_into_model(cls, model: nn.Module, state_dict: dict):\n",
    "        \"\"\"\n",
    "        Helper function to load the state dictionary into the model.\n",
    "        \"\"\"\n",
    "        state_dict = state_dict.copy()  # Avoid modifying the original state_dict\n",
    "        error_msgs = []\n",
    "\n",
    "        def load(module: torch.nn.Module, prefix: str = \"\"):\n",
    "            args = (state_dict, prefix, {}, True, [], [], error_msgs)\n",
    "            module._load_from_state_dict(*args)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + \".\")\n",
    "\n",
    "        load(model)\n",
    "        if len(error_msgs) > 0:\n",
    "            raise RuntimeError(f\"Error(s) in loading state_dict: {error_msgs}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "# EnScorePredictor Class\n",
    "class EnScorePredictor:\n",
    "    def __init__(self, whisper_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.whisper_processor = WhisperProcessor.from_pretrained(whisper_path)\n",
    "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_path).to(self.device)\n",
    "        self.whisper_tokenizer = WhisperTokenizer.from_pretrained(whisper_path)\n",
    "        self.en_token_id = self.whisper_tokenizer.convert_tokens_to_ids(\"<|en|>\")\n",
    "\n",
    "    def compute_en_score(self, audio: torch.Tensor, sample_rate: int = 16000) -> float:\n",
    "        with torch.no_grad():\n",
    "            inputs = self.whisper_processor(audio.cpu().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features.to(self.device)\n",
    "            decoder_input_ids = torch.full((input_features.shape[0], 1), 50258, dtype=torch.long, device=self.device)\n",
    "            with autocast():\n",
    "                outputs = self.whisper_model(input_features=input_features, decoder_input_ids=decoder_input_ids)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            en_score = probabilities[0, 0, self.en_token_id].item()\n",
    "        return en_score\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, dataloader, optimizer, device, en_score_predictor, epochs=CFG.epochs):\n",
    "    autoencoder.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        total_en_score = 0\n",
    "        for batch_idx, audio in enumerate(dataloader):\n",
    "            audio = audio.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed = autoencoder(audio)\n",
    "\n",
    "            # Adjust the shape to match for loss calculation\n",
    "            min_length = min(reconstructed.size(2), audio.size(2))\n",
    "            reconstructed = reconstructed[:, :, :min_length]\n",
    "            audio = audio[:, :, :min_length]\n",
    "\n",
    "            # For maximizing en_score: compute en_score on the reconstructed output\n",
    "            rec_segment = reconstructed[0, :, :min_length].detach().cpu()\n",
    "            en_score = en_score_predictor.compute_en_score(rec_segment, sample_rate=16000)\n",
    "            total_en_score += en_score\n",
    "\n",
    "            # Compute loss\n",
    "            en_score_tensor = torch.tensor(en_score, requires_grad=True, device=device)\n",
    "            loss = -en_score_tensor\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log the batch loss and en_score to W&B\n",
    "            wandb.log({\"epoch\": epoch + 1, \"batch\": batch_idx + 1, \"batch_loss\": loss.item(), \"en_score\": en_score})\n",
    "\n",
    "        # Log average loss and en_score for the epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        avg_en_score = total_en_score / len(dataloader)\n",
    "        wandb.log({\"epoch\": epoch + 1, \"epoch_loss\": avg_loss, \"avg_en_score\": avg_en_score})\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}, Avg En Score: {avg_en_score:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize W&B\n",
    "    wandb.login(key=CFG.wandb_api_key)\n",
    "    wandb.init(\n",
    "        project=CFG.project_name,\n",
    "        config={\n",
    "            \"compression_ratio\": CFG.compression_ratio,\n",
    "            \"channels\": CFG.channels,\n",
    "            \"batch_size\": CFG.batch_size,\n",
    "            \"learning_rate\": CFG.learning_rate,\n",
    "            \"epochs\": CFG.epochs,\n",
    "            \"weight_decay\": CFG.weight_decay,\n",
    "            \"dataset_path\": CFG.dataset_path,\n",
    "            \"pretrained_model_path\": CFG.pretrained_model_path,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    file_paths = glob(CFG.dataset_path)\n",
    "    # file_paths = glob(CFG.dataset_path)[:CFG.batch_size*4]\n",
    "    transform = None\n",
    "    dataset = SpeechDataset(file_paths, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Model, Optimizer, EnScorePredictor\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    autoencoder = SpeechAutoencoder.from_pretrained(\n",
    "        CFG.pretrained_model_path, compression_ratio=CFG.compression_ratio, channels=CFG.channels\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.AdamW(autoencoder.parameters(), lr=CFG.learning_rate, weight_decay=CFG.weight_decay)\n",
    "    en_score_predictor = EnScorePredictor(CFG.whisper_path, device=device)\n",
    "\n",
    "    # Train\n",
    "    train_autoencoder(autoencoder, dataloader, optimizer, device, en_score_predictor, epochs=CFG.epochs)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(autoencoder.state_dict(), CFG.model_save_path)\n",
    "\n",
    "    # Finish W&B run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T07:19:14.406603Z",
     "iopub.status.busy": "2024-12-06T07:19:14.405897Z",
     "iopub.status.idle": "2024-12-06T07:19:15.681963Z",
     "shell.execute_reply": "2024-12-06T07:19:15.681138Z",
     "shell.execute_reply.started": "2024-12-06T07:19:14.406572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "\n",
    "# Function to load and process an audio file with the autoencoder\n",
    "def process_audio(file_path, autoencoder_path, save_path=\"generated_speech.wav\"):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Display the original audio\n",
    "    print(\"Original Audio:\")\n",
    "    display(Audio(file_path, rate=sample_rate))\n",
    "\n",
    "    # Preprocessing: Convert to mono and pad for consistent input length\n",
    "    if waveform.size(0) > 1:  # If stereo, convert to mono\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.unsqueeze(0)  # Add batch dimension (1, 1, time)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the pre-trained autoencoder model\n",
    "    autoencoder = SpeechAutoencoder.from_pretrained(\n",
    "        autoencoder_path, compression_ratio=CFG.compression_ratio, channels=CFG.channels\n",
    "    ).to(device)\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Move input to the same device as the model\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    # Inference: Pass waveform through the autoencoder\n",
    "    with torch.no_grad():\n",
    "        processed_waveform = autoencoder(waveform)\n",
    "\n",
    "    # Save processed audio\n",
    "    processed_waveform = processed_waveform.squeeze(0).cpu()\n",
    "    torchaudio.save(save_path, processed_waveform, sample_rate=sample_rate)\n",
    "\n",
    "    # Display the processed audio\n",
    "    print(\"Processed Audio:\")\n",
    "    display(Audio(save_path, rate=sample_rate))\n",
    "\n",
    "    return waveform.squeeze(0).cpu(), processed_waveform, sample_rate\n",
    "\n",
    "\n",
    "# Plot waveforms and spectrograms\n",
    "def plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate):\n",
    "    # Compute Mel Spectrograms\n",
    "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n",
    "    mel_spectrogram_before = transform(waveform_before)\n",
    "    mel_spectrogram_after = transform(waveform_after)\n",
    "\n",
    "    # Plot Waveforms and Spectrograms Side-by-Side\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    # Waveform Before\n",
    "    axes[0, 0].plot(waveform_before.t().numpy())\n",
    "    axes[0, 0].set_title(\"Waveform (Before Processing)\")\n",
    "    axes[0, 0].set_xlabel(\"Time (samples)\")\n",
    "    axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "    axes[0, 0].grid()\n",
    "\n",
    "    # Waveform After\n",
    "    axes[0, 1].plot(waveform_after.t().numpy())\n",
    "    axes[0, 1].set_title(\"Waveform (After Processing)\")\n",
    "    axes[0, 1].set_xlabel(\"Time (samples)\")\n",
    "    axes[0, 1].set_ylabel(\"Amplitude\")\n",
    "    axes[0, 1].grid()\n",
    "\n",
    "    # Spectrogram Before\n",
    "    img_before = axes[1, 0].imshow(\n",
    "        mel_spectrogram_before.log2()[0, :, :].numpy(), cmap=\"viridis\", origin=\"lower\", aspect=\"auto\"\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Mel Spectrogram (Before Processing)\")\n",
    "    axes[1, 0].set_xlabel(\"Time (frames)\")\n",
    "    axes[1, 0].set_ylabel(\"Mel Frequency (bins)\")\n",
    "    fig.colorbar(img_before, ax=axes[1, 0], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Spectrogram After\n",
    "    img_after = axes[1, 1].imshow(\n",
    "        mel_spectrogram_after.log2()[0, :, :].numpy(), cmap=\"viridis\", origin=\"lower\", aspect=\"auto\"\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Mel Spectrogram (After Processing)\")\n",
    "    axes[1, 1].set_xlabel(\"Time (frames)\")\n",
    "    axes[1, 1].set_ylabel(\"Mel Frequency (bins)\")\n",
    "    fig.colorbar(img_after, ax=axes[1, 1], orientation=\"vertical\", fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    audio_files = glob(\"/input/speechocean762/train/*.wav\")\n",
    "    audio_file = audio_files[0]\n",
    "    waveform_before, waveform_after, sample_rate = process_audio(audio_file, CFG.model_save_path)\n",
    "\n",
    "    # Plot waveforms and spectrograms\n",
    "    plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T07:19:40.068630Z",
     "iopub.status.busy": "2024-12-06T07:19:40.067943Z",
     "iopub.status.idle": "2024-12-06T07:19:41.348308Z",
     "shell.execute_reply": "2024-12-06T07:19:41.347471Z",
     "shell.execute_reply.started": "2024-12-06T07:19:40.068598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_files = glob(\"/input/speechocean762/test/*.wav\")\n",
    "audio_file = audio_files[0]\n",
    "waveform_before, waveform_after, sample_rate = process_audio(audio_file, CFG.model_save_path)\n",
    "plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6195372,
     "sourceId": 10054555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6241509,
     "sourceId": 10117666,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
