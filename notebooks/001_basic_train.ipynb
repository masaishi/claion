{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import wandb\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    SpeechT5ForSpeechToSpeech,\n",
    "    SpeechT5HifiGan,\n",
    "    SpeechT5PreTrainedModel,\n",
    "    SpeechT5Processor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    WhisperTokenizer,\n",
    ")\n",
    "from transformers.models.speecht5.modeling_speecht5 import SpeechT5EncoderWithSpeechPrenet\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login(key=os.environ.get(\"WANDB_API_KEY\", \"\"))\n",
    "wandb.init(project=\"clarion-ai-t5-speech-to-speech\")\n",
    "\n",
    "# # Set CUDA memory configuration\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "\n",
    "\n",
    "def get_spectrogram_first_part(\n",
    "    model: SpeechT5PreTrainedModel,\n",
    "    input_values: torch.LongTensor,\n",
    "    speaker_embeddings: Optional[torch.FloatTensor],\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    ") -> Tuple[torch.FloatTensor, torch.LongTensor, torch.FloatTensor, int, int]:\n",
    "    \"\"\"First part of the spectrogram generation to reduce complexity.\"\"\"\n",
    "    input_values = input_values.to(model.device, dtype=torch.half)\n",
    "    if speaker_embeddings is not None:\n",
    "        speaker_embeddings = speaker_embeddings.to(model.device, dtype=torch.half)\n",
    "\n",
    "    bsz = input_values.size(0)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        encoder_attention_mask = 1 - (input_values == model.config.pad_token_id).int()\n",
    "    else:\n",
    "        encoder_attention_mask = attention_mask\n",
    "\n",
    "    encoder_out = model.speecht5.encoder(\n",
    "        input_values=input_values,\n",
    "        attention_mask=encoder_attention_mask,\n",
    "        return_dict=True,\n",
    "    )\n",
    "\n",
    "    encoder_last_hidden_state = encoder_out.last_hidden_state\n",
    "\n",
    "    # downsample encoder attention mask\n",
    "    if isinstance(model.speecht5.encoder, SpeechT5EncoderWithSpeechPrenet):\n",
    "        encoder_attention_mask = model.speecht5.encoder.prenet._get_feature_vector_attention_mask(\n",
    "            encoder_out[0].shape[1], encoder_attention_mask\n",
    "        )\n",
    "\n",
    "    return encoder_last_hidden_state, encoder_attention_mask, speaker_embeddings, bsz\n",
    "\n",
    "\n",
    "def get_spectrogram(\n",
    "    model: SpeechT5PreTrainedModel,\n",
    "    input_values: torch.LongTensor,\n",
    "    speaker_embeddings: Optional[torch.FloatTensor],\n",
    "    attention_mask: Optional[torch.LongTensor] = None,\n",
    "    threshold: float = 0.5,\n",
    "    minlenratio: float = 0.0,\n",
    "    maxlenratio: float = 20.0,\n",
    "    output_cross_attentions: bool = False,\n",
    "    return_output_lengths: bool = False,\n",
    ") -> Union[torch.FloatTensor, Tuple[torch.FloatTensor, torch.FloatTensor]]:\n",
    "    \"\"\"Generate spectrogram from input values.\"\"\"\n",
    "    # Get initial setup and encoder outputs\n",
    "    encoder_last_hidden_state, encoder_attention_mask, speaker_embeddings, bsz = get_spectrogram_first_part(\n",
    "        model, input_values, speaker_embeddings, attention_mask\n",
    "    )\n",
    "\n",
    "    maxlen = int(encoder_last_hidden_state.size(1) * maxlenratio / model.config.reduction_factor)\n",
    "    minlen = int(encoder_last_hidden_state.size(1) * minlenratio / model.config.reduction_factor)\n",
    "\n",
    "    # Start the output sequence with a mel spectrum that is all zeros.\n",
    "    output_sequence = encoder_last_hidden_state.new_zeros(bsz, 1, model.config.num_mel_bins)\n",
    "\n",
    "    spectrogram = []\n",
    "    cross_attentions = []\n",
    "    past_key_values = None\n",
    "    idx = 0\n",
    "    result_spectrogram = {}\n",
    "\n",
    "    return process_spectrogram_loop(\n",
    "        model,\n",
    "        encoder_last_hidden_state,\n",
    "        encoder_attention_mask,\n",
    "        output_sequence,\n",
    "        speaker_embeddings,\n",
    "        bsz,\n",
    "        spectrogram,\n",
    "        cross_attentions,\n",
    "        past_key_values,\n",
    "        idx,\n",
    "        result_spectrogram,\n",
    "        minlen,\n",
    "        maxlen,\n",
    "        threshold,\n",
    "        output_cross_attentions,\n",
    "        return_output_lengths,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_spectrogram_loop(\n",
    "    model,\n",
    "    encoder_last_hidden_state,\n",
    "    encoder_attention_mask,\n",
    "    output_sequence,\n",
    "    speaker_embeddings,\n",
    "    bsz,\n",
    "    spectrogram,\n",
    "    cross_attentions,\n",
    "    past_key_values,\n",
    "    idx,\n",
    "    result_spectrogram,\n",
    "    minlen,\n",
    "    maxlen,\n",
    "    threshold,\n",
    "    output_cross_attentions,\n",
    "    return_output_lengths,\n",
    "):\n",
    "    \"\"\"Process the spectrogram generation loop to reduce complexity.\"\"\"\n",
    "    while True:\n",
    "        idx += 1\n",
    "\n",
    "        # Run the decoder prenet on the entire output sequence.\n",
    "        decoder_hidden_states = model.speecht5.decoder.prenet(output_sequence, speaker_embeddings)\n",
    "        # Run the decoder layers on the last element of the prenet output.\n",
    "        decoder_out = model.speecht5.decoder.wrapped_decoder(\n",
    "            hidden_states=decoder_hidden_states[:, -1:],\n",
    "            attention_mask=None,\n",
    "            encoder_hidden_states=encoder_last_hidden_state,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=True,\n",
    "            output_attentions=output_cross_attentions,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        if output_cross_attentions:\n",
    "            cross_attentions.append(torch.cat(decoder_out.cross_attentions, dim=0))\n",
    "\n",
    "        last_decoder_output = decoder_out.last_hidden_state.squeeze(1)\n",
    "        past_key_values = decoder_out.past_key_values\n",
    "\n",
    "        # Predict the new mel spectrum for this step in the sequence.\n",
    "        spectrum = model.speech_decoder_postnet.feat_out(last_decoder_output)\n",
    "        spectrum = spectrum.view(bsz, model.config.reduction_factor, model.config.num_mel_bins)\n",
    "        spectrogram.append(spectrum)\n",
    "\n",
    "        # Extend the output sequence with the new mel spectrum.\n",
    "        new_spectrogram = spectrum[:, -1, :].view(bsz, 1, model.config.num_mel_bins)\n",
    "        output_sequence = torch.cat((output_sequence, new_spectrogram), dim=1)\n",
    "        # Predict the probability that this is the stop token.\n",
    "        prob = torch.sigmoid(model.speech_decoder_postnet.prob_out(last_decoder_output))\n",
    "\n",
    "        if idx < minlen:\n",
    "            continue\n",
    "\n",
    "        # If the generation loop is less than maximum length time, check the ones in the batch that have met\n",
    "        # the prob threshold. Otherwise, assume all have met thresholds and fill other spectrograms for the batch.\n",
    "        if idx < maxlen:\n",
    "            meet_thresholds = torch.sum(prob, dim=-1) >= threshold\n",
    "            meet_indexes = torch.where(meet_thresholds)[0].tolist()\n",
    "        else:\n",
    "            meet_indexes = range(len(prob))\n",
    "        meet_indexes = [i for i in meet_indexes if i not in result_spectrogram]\n",
    "\n",
    "        if len(meet_indexes) > 0:\n",
    "            spectrograms = torch.stack(spectrogram)\n",
    "            spectrograms = spectrograms.transpose(0, 1).flatten(1, 2)\n",
    "            spectrograms = model.speech_decoder_postnet.postnet(spectrograms)\n",
    "            for meet_index in meet_indexes:\n",
    "                result_spectrogram[meet_index] = spectrograms[meet_index]\n",
    "\n",
    "        if len(result_spectrogram) >= bsz:\n",
    "            break\n",
    "\n",
    "    spectrograms = [result_spectrogram[i] for i in range(len(result_spectrogram))]\n",
    "    if not return_output_lengths:\n",
    "        spectrogram = spectrograms[0] if bsz == 1 else torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
    "    return spectrogram\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=1.0):\n",
    "    \"\"\"Clips gradients to prevent exploding gradients.\"\"\"\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "\n",
    "class SpeechToSpeechTrainer:\n",
    "    def __init__(self, model_path: str, vocoder_path: str, whisper_path: str, device: str):\n",
    "        self.device = device\n",
    "        self.processor = SpeechT5Processor.from_pretrained(model_path)\n",
    "        self.model = SpeechT5ForSpeechToSpeech.from_pretrained(model_path).to(device)\n",
    "        self.model.config.use_cache = False\n",
    "        self.model.speecht5.decoder.forward = partial(self.model.speecht5.decoder.forward, use_cache=True)\n",
    "        self.vocoder = SpeechT5HifiGan.from_pretrained(vocoder_path).to(device)\n",
    "        self.whisper_processor = WhisperProcessor.from_pretrained(whisper_path)\n",
    "        self.whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_path).to(device)\n",
    "        self.whisper_tokenizer = WhisperTokenizer.from_pretrained(whisper_path)\n",
    "        self.en_token_id = self.whisper_tokenizer.convert_tokens_to_ids(\"<|en|>\")\n",
    "\n",
    "    def compute_en_score(self, audio: torch.Tensor, sample_rate: int = 16000) -> float:\n",
    "        with torch.no_grad():\n",
    "            inputs = self.whisper_processor(audio.cpu().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "            input_features = inputs.input_features.to(self.device)\n",
    "            decoder_input_ids = torch.full((input_features.shape[0], 1), 50258, dtype=torch.long, device=self.device)\n",
    "            with autocast():  # Mixed precision inference\n",
    "                outputs = self.whisper_model(input_features=input_features, decoder_input_ids=decoder_input_ids)\n",
    "            probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            en_score = probabilities[0, 0, self.en_token_id].item()\n",
    "        return en_score\n",
    "\n",
    "    def train(self, audio_files, optimizer, num_epochs=3, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.model.speecht5.encoder.train()\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        scaler = GradScaler()\n",
    "        accumulation_steps = 2\n",
    "        mean_steps = 10\n",
    "        step_scores = []\n",
    "        top_models = []  # Store top models with their mean en_score\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            torch.cuda.empty_cache()\n",
    "            epoch_loss = 0.0\n",
    "            current_scores = []  # Track scores for logging\n",
    "\n",
    "            for step, file_path in enumerate(tqdm(audio_files, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
    "                waveform, sample_rate = torchaudio.load(file_path)\n",
    "                waveform = waveform.to(self.device, dtype=torch.float)\n",
    "\n",
    "                inputs = self.processor(\n",
    "                    audio=waveform.squeeze().cpu().numpy(), sampling_rate=sample_rate, return_tensors=\"pt\"\n",
    "                )\n",
    "                input_values = inputs[\"input_values\"].to(self.device, dtype=torch.float)\n",
    "                speaker_embeddings = torch.zeros((1, 512), device=self.device, dtype=torch.float)\n",
    "\n",
    "                with autocast():\n",
    "                    spectrogram = get_spectrogram(self.model, input_values, speaker_embeddings)\n",
    "                    generated_audio = self.vocoder(spectrogram).unsqueeze(0).to(self.device)\n",
    "\n",
    "                    # Compute English score\n",
    "                    en_score = self.compute_en_score(generated_audio)\n",
    "                    step_scores.append(en_score)\n",
    "                    current_scores.append(en_score)\n",
    "\n",
    "                    # Define loss\n",
    "                    target_waveform = waveform.to(self.device)\n",
    "                    target_length = min(generated_audio.size(1), target_waveform.size(1))\n",
    "                    mse_loss = torch.nn.functional.mse_loss(\n",
    "                        generated_audio[:, :target_length], target_waveform[:, :target_length]\n",
    "                    )\n",
    "                    en_loss = 1 - en_score**2\n",
    "                    loss = (mse_loss + 5 * en_loss) / accumulation_steps  # Balanced loss\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                # Clip gradients\n",
    "                clip_gradients(self.model.speecht5.encoder)\n",
    "\n",
    "                if (step + 1) % accumulation_steps == 0 or (step + 1) == len(audio_files):\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                epoch_loss += loss.item() * accumulation_steps\n",
    "\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"step\": step + 1,\n",
    "                        \"loss\": loss.item(),\n",
    "                        \"en_score\": en_score,\n",
    "                        \"mse_loss\": mse_loss.item(),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if (step + 1) % mean_steps == 0:\n",
    "                    mean_score = sum(current_scores) / len(current_scores)\n",
    "                    wandb.log({f\"{mean_steps}_step_mean_en_score\": mean_score, \"step\": step + 1})\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        checkpoint_dir, f\"model_epoch_{epoch + 1}_step_{step + 1}_mean_score_{mean_score:.4f}.pt\"\n",
    "                    )\n",
    "                    torch.save(self.model.speecht5.encoder.state_dict(), checkpoint_path)\n",
    "                    top_models.append((mean_score, checkpoint_path))\n",
    "                    top_models = sorted(top_models, key=lambda x: x[0], reverse=True)[:3]\n",
    "                    current_scores = []\n",
    "\n",
    "            avg_loss = epoch_loss / len(audio_files)\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch + 1} completed. Avg Loss: {avg_loss:.4f}\")\n",
    "            wandb.log({\"epoch_loss\": avg_loss, \"epoch\": epoch + 1, \"learning_rate\": scheduler.get_last_lr()[0]})\n",
    "\n",
    "        return self.model\n",
    "\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    num_epochs = 1\n",
    "\n",
    "    # Load audio files\n",
    "    audio_files = glob(\"/input/speechocean762/train/*.wav\")\n",
    "    if not audio_files:\n",
    "        raise ValueError(\"No audio files found in the specified path.\")\n",
    "\n",
    "    # Initialize components\n",
    "    model_path = \"microsoft/speecht5_vc\"\n",
    "    vocoder_path = \"microsoft/speecht5_hifigan\"\n",
    "    whisper_path = \"openai/whisper-tiny\"\n",
    "    trainer = SpeechToSpeechTrainer(\n",
    "        model_path=model_path, vocoder_path=vocoder_path, whisper_path=whisper_path, device=device\n",
    "    )\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(trainer.model.speecht5.encoder.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = trainer.train(audio_files, optimizer, num_epochs=num_epochs, checkpoint_dir=\"./checkpoints\")\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = \"./final_speech_to_speech_model.pt\"\n",
    "    torch.save(trained_model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved at {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-04T08:14:33.179244Z",
     "iopub.status.idle": "2024-12-04T08:14:33.179572Z",
     "shell.execute_reply": "2024-12-04T08:14:33.179428Z",
     "shell.execute_reply.started": "2024-12-04T08:14:33.179412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import Audio, display\n",
    "\n",
    "# display(Audio(\"/working/bef.wav\", rate=16000))\n",
    "# display(Audio(\"/working/gen.wav\", rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-04T08:14:33.180448Z",
     "iopub.status.idle": "2024-12-04T08:14:33.180755Z",
     "shell.execute_reply": "2024-12-04T08:14:33.180628Z",
     "shell.execute_reply.started": "2024-12-04T08:14:33.180613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoints = glob(\"/working/checkpoints/*\")\n",
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-04T08:14:33.181869Z",
     "iopub.status.idle": "2024-12-04T08:14:33.182182Z",
     "shell.execute_reply": "2024-12-04T08:14:33.182043Z",
     "shell.execute_reply.started": "2024-12-04T08:14:33.182007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Load the state dictionaries\n",
    "state_dict_1 = torch.load(checkpoints[0], weights_only=True)\n",
    "state_dict_2 = torch.load(checkpoints[1], weights_only=True)\n",
    "\n",
    "are_equal = all(torch.equal(state_dict_1[key], state_dict_2[key]) for key in state_dict_1.keys())\n",
    "\n",
    "print(f\"Are the state dictionaries equal? {are_equal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-04T08:14:33.183438Z",
     "iopub.status.idle": "2024-12-04T08:14:33.183714Z",
     "shell.execute_reply": "2024-12-04T08:14:33.183597Z",
     "shell.execute_reply.started": "2024-12-04T08:14:33.183583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6195372,
     "sourceId": 10054555,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
