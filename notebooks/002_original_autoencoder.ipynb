{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:04:33.268028Z",
     "iopub.status.busy": "2024-12-06T06:04:33.267164Z",
     "iopub.status.idle": "2024-12-06T06:04:33.271784Z",
     "shell.execute_reply": "2024-12-06T06:04:33.270857Z",
     "shell.execute_reply.started": "2024-12-06T06:04:33.267993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CFG:\n",
    "    compression_ratio = 0.9\n",
    "    channels = 16\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-4\n",
    "    epochs = 20\n",
    "    project_name = \"clarion-ai-002\"\n",
    "    wandb_api_key = os.environ.get(\"WANDB_API_KEY\", \"\")\n",
    "    dataset_path = \"/input/speechocean762/train/*.wav\"\n",
    "    model_save_path = \"speech_autoencoder.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T06:05:15.307578Z",
     "iopub.status.busy": "2024-12-06T06:05:15.307195Z",
     "iopub.status.idle": "2024-12-06T06:05:22.227735Z",
     "shell.execute_reply": "2024-12-06T06:05:22.226840Z",
     "shell.execute_reply.started": "2024-12-06T06:05:15.307538Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "import wandb\n",
    "\n",
    "def collate_fn(batch):\n",
    "    lengths = [item.size(1) for item in batch if item.numel() > 0]  # Skip empty tensors\n",
    "    max_length = max(lengths)\n",
    "    padded_batch = torch.zeros(len(batch), 1, max_length, dtype=batch[0].dtype)\n",
    "    \n",
    "    for i, item in enumerate(batch):\n",
    "        if item.numel() > 0:\n",
    "            padded_batch[i, :, :item.size(1)] = item\n",
    "    \n",
    "    return padded_batch\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, sample_rate = torchaudio.load(self.file_paths[idx])\n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        return audio\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, compression_ratio=0.7, channels=16):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.compression_ratio = compression_ratio\n",
    "\n",
    "        # Three convolutional layers with group normalization and ReLU activation\n",
    "        # Increase the number of feature maps to improve representational capacity\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            nn.Conv1d(1, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv1d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv1d(channels, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the convolutional layers\n",
    "        x = self.encoder_layers(x)\n",
    "        # Compress the time dimension\n",
    "        compressed_length = int(x.size(2) * self.compression_ratio)\n",
    "        x = nn.functional.interpolate(x, size=compressed_length, mode='linear', align_corners=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, expansion_ratio=1/0.7, channels=16):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.expansion_ratio = expansion_ratio\n",
    "\n",
    "        # Three transposed convolutional layers with group normalization and ReLU activation\n",
    "        # Mirror the encoder structure\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            nn.ConvTranspose1d(1, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose1d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose1d(channels, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(1, 1)\n",
    "            # Note: Final activation can be omitted or chosen based on the output signal's nature\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand the time dimension\n",
    "        original_length = int(x.size(2) * self.expansion_ratio)\n",
    "        x = nn.functional.interpolate(x, size=original_length, mode='linear', align_corners=False)\n",
    "        # Pass through the transposed convolutional layers\n",
    "        x = self.decoder_layers(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "class SpeechAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, compression_ratio=0.7, channels=16):\n",
    "        super(SpeechAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(compression_ratio=compression_ratio, channels=channels)\n",
    "        self.decoder = Decoder(expansion_ratio=1/compression_ratio, channels=channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        \n",
    "        # print(x.shape, x.numel())\n",
    "        # print(latent.shape, latent.numel())\n",
    "        # print(reconstructed.shape, reconstructed.numel())\n",
    "        # print(\"-\"*30)\n",
    "\n",
    "        # Adjust the shape of the reconstructed signal to match the input\n",
    "        if reconstructed.size(2) > x.size(2):\n",
    "            # If the output is longer, trim it\n",
    "            reconstructed = reconstructed[:, :, :x.size(2)]\n",
    "        elif reconstructed.size(2) < x.size(2):\n",
    "            # If the output is shorter, pad it\n",
    "            pad_length = x.size(2) - reconstructed.size(2)\n",
    "            reconstructed = nn.functional.pad(reconstructed, (0, pad_length))\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_path, device=\"cpu\", **kwargs):\n",
    "        init_params = inspect.signature(cls).parameters\n",
    "        init_kwargs = {\n",
    "            key: param.default\n",
    "            for key, param in init_params.items()\n",
    "            if param.default is not inspect.Parameter.empty and key != \"self\"\n",
    "        }\n",
    "        init_kwargs.update(kwargs)\n",
    "        model = cls(**init_kwargs).to(device)\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _load_state_dict_into_model(cls, model: nn.Module, state_dict: dict):\n",
    "        \"\"\"\n",
    "        Helper function to load the state dictionary into the model.\n",
    "        \"\"\"\n",
    "        state_dict = state_dict.copy()  # Avoid modifying the original state_dict\n",
    "        error_msgs = []\n",
    "\n",
    "        def load(module: torch.nn.Module, prefix: str = \"\"):\n",
    "            args = (state_dict, prefix, {}, True, [], [], error_msgs)\n",
    "            module._load_from_state_dict(*args)\n",
    "            for name, child in module._modules.items():\n",
    "                if child is not None:\n",
    "                    load(child, prefix + name + \".\")\n",
    "\n",
    "        load(model)\n",
    "        if len(error_msgs) > 0:\n",
    "            raise RuntimeError(f\"Error(s) in loading state_dict: {error_msgs}\")\n",
    "        return model\n",
    "\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, reconstructed, original):\n",
    "        abs_diff = torch.abs(reconstructed - original)\n",
    "        quadratic = torch.where(abs_diff <= self.delta, 0.5 * abs_diff ** 2, self.delta * (abs_diff - 0.5 * self.delta))\n",
    "        return quadratic.mean()\n",
    "        \n",
    "def train_autoencoder(autoencoder, dataloader, optimizer, device, epochs=10):\n",
    "    autoencoder.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        criterion = nn.MSELoss()\n",
    "        for batch_idx, audio in enumerate(dataloader):\n",
    "            audio = audio.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = autoencoder(audio)\n",
    "            \n",
    "            # Adjust the shape to match for loss calculation\n",
    "            min_length = min(reconstructed.size(2), audio.size(2))\n",
    "            reconstructed = reconstructed[:, :, :min_length]\n",
    "            audio = audio[:, :, :min_length]\n",
    "            \n",
    "            # Compute reconstruction loss\n",
    "            loss = criterion(reconstructed, audio)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log the batch loss to W&B\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx + 1,\n",
    "                \"batch_loss\": loss.item()\n",
    "            })\n",
    "        \n",
    "        # Log the average loss for the epoch\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        wandb.log({\"epoch\": epoch + 1, \"epoch_loss\": avg_loss})\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Log parameters to W&B\n",
    "wandb.login(key=CFG.wandb_api_key)\n",
    "wandb.init(project=CFG.project_name, config={\n",
    "    \"compression_ratio\": CFG.compression_ratio,\n",
    "    \"channels\": CFG.channels,\n",
    "    \"batch_size\": CFG.batch_size,\n",
    "    \"learning_rate\": CFG.learning_rate,\n",
    "    \"epochs\": CFG.epochs\n",
    "})\n",
    "\n",
    "# Dataset and DataLoader\n",
    "file_paths = glob(CFG.dataset_path)\n",
    "# file_paths = glob(CFG.dataset_path)[:32]\n",
    "transform = None\n",
    "dataset = SpeechDataset(file_paths, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Model and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "autoencoder = SpeechAutoencoder(\n",
    "    compression_ratio=CFG.compression_ratio, \n",
    "    channels=CFG.channels\n",
    ").to(device)\n",
    "print(autoencoder)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    autoencoder.parameters(), \n",
    "    lr=CFG.learning_rate, \n",
    "    weight_decay=CFG.weight_decay\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "train_autoencoder(autoencoder, dataloader, optimizer, device, CFG.epochs)\n",
    "\n",
    "# Save the model\n",
    "torch.save(autoencoder.state_dict(), CFG.model_save_path)\n",
    "\n",
    "# Finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T15:05:25.827686Z",
     "iopub.status.busy": "2024-12-05T15:05:25.826936Z",
     "iopub.status.idle": "2024-12-05T15:05:25.835167Z",
     "shell.execute_reply": "2024-12-05T15:05:25.834486Z",
     "shell.execute_reply.started": "2024-12-05T15:05:25.827646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch.save(autoencoder.state_dict(), \"speech_autoencoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T18:18:33.272900Z",
     "iopub.status.busy": "2024-12-05T18:18:33.272095Z",
     "iopub.status.idle": "2024-12-05T18:18:34.639638Z",
     "shell.execute_reply": "2024-12-05T18:18:34.638745Z",
     "shell.execute_reply.started": "2024-12-05T18:18:33.272865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Function to load and process an audio file with the autoencoder\n",
    "def process_audio(file_path, autoencoder_path, save_path=\"generated_speech.wav\"):\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Display the original audio\n",
    "    print(\"Original Audio:\")\n",
    "    display(Audio(file_path, rate=sample_rate))\n",
    "\n",
    "    # Preprocessing: Convert to mono and pad for consistent input length\n",
    "    if waveform.size(0) > 1:  # If stereo, convert to mono\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    waveform = waveform.unsqueeze(0)  # Add batch dimension (1, 1, time)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load the pre-trained autoencoder model\n",
    "    autoencoder = SpeechAutoencoder.from_pretrained(CFG.model_save_path, compression_ratio=CFG.compression_ratio, channels=CFG.channels).to(device)\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # Move input to the same device as the model\n",
    "    waveform = waveform.to(device)\n",
    "\n",
    "    # Inference: Pass waveform through the autoencoder\n",
    "    with torch.no_grad():\n",
    "        processed_waveform = autoencoder(waveform)\n",
    "\n",
    "    # Save processed audio\n",
    "    processed_waveform = processed_waveform.squeeze(0).cpu()  # Remove batch dimension\n",
    "    torchaudio.save(save_path, processed_waveform, sample_rate=sample_rate)\n",
    "\n",
    "    # Display the processed audio\n",
    "    print(\"Processed Audio:\")\n",
    "    display(Audio(save_path, rate=sample_rate))\n",
    "\n",
    "    return waveform.squeeze(0).cpu(), processed_waveform, sample_rate\n",
    "\n",
    "# Plot waveforms and spectrograms\n",
    "def plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate):\n",
    "    # Compute Mel Spectrograms\n",
    "    transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=64)\n",
    "    mel_spectrogram_before = transform(waveform_before)\n",
    "    mel_spectrogram_after = transform(waveform_after)\n",
    "\n",
    "    # Plot Waveforms and Spectrograms Side-by-Side\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    # Waveform Before\n",
    "    axes[0, 0].plot(waveform_before.t().numpy())\n",
    "    axes[0, 0].set_title(\"Waveform (Before Processing)\")\n",
    "    axes[0, 0].set_xlabel(\"Time (samples)\")\n",
    "    axes[0, 0].set_ylabel(\"Amplitude\")\n",
    "    axes[0, 0].grid()\n",
    "\n",
    "    # Waveform After\n",
    "    axes[0, 1].plot(waveform_after.t().numpy())\n",
    "    axes[0, 1].set_title(\"Waveform (After Processing)\")\n",
    "    axes[0, 1].set_xlabel(\"Time (samples)\")\n",
    "    axes[0, 1].set_ylabel(\"Amplitude\")\n",
    "    axes[0, 1].grid()\n",
    "\n",
    "    # Spectrogram Before\n",
    "    img_before = axes[1, 0].imshow(\n",
    "        mel_spectrogram_before.log2()[0, :, :].numpy(),\n",
    "        cmap='viridis',\n",
    "        origin='lower',\n",
    "        aspect='auto'\n",
    "    )\n",
    "    axes[1, 0].set_title(\"Mel Spectrogram (Before Processing)\")\n",
    "    axes[1, 0].set_xlabel(\"Time (frames)\")\n",
    "    axes[1, 0].set_ylabel(\"Mel Frequency (bins)\")\n",
    "    fig.colorbar(img_before, ax=axes[1, 0], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Spectrogram After\n",
    "    img_after = axes[1, 1].imshow(\n",
    "        mel_spectrogram_after.log2()[0, :, :].numpy(),\n",
    "        cmap='viridis',\n",
    "        origin='lower',\n",
    "        aspect='auto'\n",
    "    )\n",
    "    axes[1, 1].set_title(\"Mel Spectrogram (After Processing)\")\n",
    "    axes[1, 1].set_xlabel(\"Time (frames)\")\n",
    "    axes[1, 1].set_ylabel(\"Mel Frequency (bins)\")\n",
    "    fig.colorbar(img_after, ax=axes[1, 1], orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    audio_files = glob(\"/input/speechocean762/train/*.wav\")\n",
    "    audio_file = audio_files[0]\n",
    "    waveform_before, waveform_after, sample_rate = process_audio(audio_file, CFG.model_save_path)\n",
    "\n",
    "    # Plot waveforms and spectrograms\n",
    "    plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T18:16:00.677937Z",
     "iopub.status.busy": "2024-12-05T18:16:00.677598Z",
     "iopub.status.idle": "2024-12-05T18:16:00.694209Z",
     "shell.execute_reply": "2024-12-05T18:16:00.693386Z",
     "shell.execute_reply.started": "2024-12-05T18:16:00.677908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "audio_files = glob(\"/input/speechocean762/test/*.wav\")\n",
    "audio_file = audio_files[0]\n",
    "waveform_before, waveform_after, sample_rate = process_audio(audio_file, CFG.model_save_path)\n",
    "plot_waveforms_and_spectrograms(waveform_before, waveform_after, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6195372,
     "sourceId": 10054555,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6241509,
     "sourceId": 10117486,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
